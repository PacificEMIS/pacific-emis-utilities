{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb12f95-19d7-4f24-802b-166d5d631cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os  # Importing os to work with paths\n",
    "import json\n",
    "\n",
    "def load_config(config_path=\"config.json\"):\n",
    "    \"\"\"Load configuration from a JSON file.\"\"\"\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = json.load(file)\n",
    "    return config['un_pop_division_api_token']\n",
    "\n",
    "# Test loading configuration\n",
    "un_pop_division_api_token = load_config()\n",
    "print(\"Configuration loaded successfully.\")\n",
    "\n",
    "# Define the folder path and file name separately\n",
    "folder_path = \"/mnt/h/Development/Pacific EMIS/repositories-data/pacific-emis-utilities/RMI\"\n",
    "\n",
    "# Store the API Bearer Token (Replace 'your_token_here' with the actual token)\n",
    "API_TOKEN = un_pop_division_api_token\n",
    "\n",
    "# Define the base URL\n",
    "BASE_URL = \"https://population.un.org/dataportalapi/api/v1/data/\"\n",
    "\n",
    "# Define headers to use in all requests\n",
    "HEADERS = {\n",
    "    'Authorization': f'Bearer {API_TOKEN}'\n",
    "}\n",
    "\n",
    "print(\"API Token and Headers set successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31113888-916b-493f-a660-fc62033bd4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Base URL for retrieving indicators\n",
    "base_url = \"https://population.un.org/dataportalapi/api/v1/indicators\"\n",
    "\n",
    "# Initialize list to store all indicators (ID and Name)\n",
    "all_indicators = []\n",
    "\n",
    "# Start with the first page\n",
    "page_number = 1\n",
    "\n",
    "while True:\n",
    "    # Make the API request with pagination\n",
    "    response = requests.get(base_url, params={\"pageNumber\": page_number})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        \n",
    "        # Extract ID and Name from the \"data\" field\n",
    "        if \"data\" in data:\n",
    "            all_indicators.extend([(item[\"id\"], item[\"name\"]) for item in data[\"data\"]])\n",
    "\n",
    "        # Check if there is another page\n",
    "        if data.get(\"nextPage\"):\n",
    "            page_number += 1  # Move to the next page\n",
    "        else:\n",
    "            break  # No more pages, exit loop\n",
    "    else:\n",
    "        print(f\"Error retrieving indicators: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "# Display each Indicator ID and Name side by side\n",
    "print(\"Sample indicators:\")\n",
    "for ind_id, ind_name in all_indicators[:5]:\n",
    "    print(f\"{ind_id}: {ind_name}\")\n",
    "\n",
    "print(\"\\nThe one we want:\")\n",
    "indicator_47 = next((name for ind_id, name in all_indicators if ind_id == 47), \"Indicator ID 47 not found\")\n",
    "print(f\"\\n47: {indicator_47}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a208ad-9abc-4d5e-92b4-71edc034f931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Details about indicators\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Declares the base url for calling the API\n",
    "base_url = \"https://population.un.org/dataportalapi/api/v1\"\n",
    "\n",
    "# Creates the target URL, indicators, in this instance\n",
    "target = base_url + \"/indicators/\"\n",
    "\n",
    "# Get the response, which includes the first page of data as well as information on pagination and number of records\n",
    "response = requests.get(target)\n",
    "\n",
    "# Converts call into JSON\n",
    "j = response.json()\n",
    "\n",
    "# Converts JSON into a pandas DataFrame.\n",
    "df = pd.json_normalize(j['data']) # pd.json_normalize flattens the JSON to accomodate nested lists within the JSON structure\n",
    "\n",
    "# Loop until there are new pages with data\n",
    "while j['nextPage'] != None:\n",
    "    # Reset the target to the next page\n",
    "    target = j['nextPage']\n",
    "\n",
    "    #call the API for the next page\n",
    "    response = requests.get(target)\n",
    "\n",
    "    # Convert response to JSON format\n",
    "    j = response.json()\n",
    "\n",
    "    # Store the next page in a data frame\n",
    "    df_temp = pd.json_normalize(j['data'])\n",
    "\n",
    "    # Append next page to the data frame\n",
    "    df = df.append(df_temp)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91224739-95e7-4c1a-8aaf-feeeb40824d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Base URL for retrieving locations\n",
    "base_url = \"https://population.un.org/dataportalapi/api/v1/locations\"\n",
    "\n",
    "# Initialize list to store all locations (ID and Name)\n",
    "all_locations = []\n",
    "\n",
    "# Start with the first page\n",
    "page_number = 1\n",
    "\n",
    "while True:\n",
    "    # Make the API request with pagination\n",
    "    response = requests.get(base_url, params={\"pageNumber\": page_number, \"sort\": \"id\"})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        \n",
    "        # Extract ID and Name from the \"data\" field\n",
    "        if \"data\" in data:\n",
    "            all_locations.extend([(item[\"id\"], item[\"name\"]) for item in data[\"data\"]])\n",
    "\n",
    "        # Check if there is another page\n",
    "        if data.get(\"nextPage\"):\n",
    "            page_number += 1  # Move to the next page\n",
    "        else:\n",
    "            break  # No more pages, exit loop\n",
    "    else:\n",
    "        print(f\"Error retrieving locations: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "# Display each Location ID and Name side by side\n",
    "print(\"Sample countries:\")\n",
    "for loc_id, loc_name in all_locations[:5]:\n",
    "    print(f\"{loc_id}: {loc_name}\")\n",
    "\n",
    "print(\"\\nThe one we want:\")\n",
    "location_584 = next((name for loc_id, name in all_locations if loc_id == 584), \"Location ID 584 not found\")\n",
    "print(f\"\\n47: {location_584}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91bbee4-7f78-42dc-bf57-9b56bb36a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Define file paths (adjust folder_path as needed)\n",
    "parquet_file_name = \"population_data.parquet\"\n",
    "parquet_file_path = os.path.join(folder_path, parquet_file_name)\n",
    "excel_file_name = \"population_data.xlsx\"\n",
    "excel_file_path = os.path.join(folder_path, excel_file_name)\n",
    "\n",
    "# Check if the parquet file already exists\n",
    "if os.path.exists(parquet_file_path):\n",
    "    print(\"Loading data from local parquet file...\")\n",
    "    df = pd.read_parquet(parquet_file_path, engine=\"pyarrow\")\n",
    "else:\n",
    "    print(\"Local data not found. Retrieving data from API...\")\n",
    "    # Define parameters for the API request\n",
    "    indicator_id = 47   # Population by age and sex\n",
    "    location_id = 584   # Marshall Islands\n",
    "    start_year = 2011\n",
    "    end_year = 2030\n",
    "\n",
    "    api_url = f\"{BASE_URL}indicators/{indicator_id}/locations/{location_id}/start/{start_year}/end/{end_year}\"\n",
    "\n",
    "    # Initialize an empty list to collect all data and set the page number to start at 1\n",
    "    all_data = []\n",
    "    page_number = 1\n",
    "\n",
    "    while True:\n",
    "        # Construct API request with paging\n",
    "        paged_url = f\"{api_url}?pageNumber={page_number}\"\n",
    "        print(f\"Retrieving from {paged_url}\")\n",
    "        \n",
    "        response = requests.get(paged_url, headers=HEADERS)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            \n",
    "            # Extract data if present\n",
    "            if \"data\" in data:\n",
    "                all_data.extend(data[\"data\"])  # Append new data\n",
    "            \n",
    "            # Check if there are more pages\n",
    "            if data.get(\"nextPage\"):\n",
    "                page_number += 1  # Move to the next page\n",
    "            else:\n",
    "                break  # No more pages, exit loop\n",
    "        else:\n",
    "            print(\"Error retrieving data:\", response.status_code, response.text)\n",
    "            break\n",
    "\n",
    "    # Convert all collected data into a DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "    print(f\"Total records retrieved: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    \n",
    "    # Save the DataFrame locally as both parquet and Excel files\n",
    "    print(f\"Saving data to parquet at: {parquet_file_path}\")\n",
    "    df.to_parquet(parquet_file_path, engine=\"pyarrow\")\n",
    "    print(f\"Saving data to Excel at: {excel_file_path}\")\n",
    "    df.to_excel(excel_file_path, index=False)\n",
    "\n",
    "# Now df is loaded either from the local file or the API, and you can proceed with further analysis.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74cd02e-66aa-430e-9268-81cde93988c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Create a dynamic mapping for popmodCode ---\n",
    "# Get unique variants from variantShortName\n",
    "unique_variants = df[\"variantShortName\"].unique().tolist()\n",
    "\n",
    "mapping = {}\n",
    "if \"Median\" in unique_variants:\n",
    "    # Ensure \"Median\" is first\n",
    "    mapping[\"Median\"] = \"UNPD24V1\"\n",
    "    remaining = [v for v in unique_variants if v != \"Median\"]\n",
    "else:\n",
    "    remaining = unique_variants\n",
    "\n",
    "# Sort the remaining values (alphabetical order)\n",
    "remaining = sorted(remaining)\n",
    "\n",
    "# Start numbering at 2 if Median was found, otherwise start at 1\n",
    "start_index = 2 if \"Median\" in mapping else 1\n",
    "for i, variant in enumerate(remaining, start=start_index):\n",
    "    mapping[variant] = f\"UNPD24V{i}\"\n",
    "\n",
    "# Create the new popmodCode column using the dynamic mapping\n",
    "df[\"popmodCode\"] = df[\"variantShortName\"].map(mapping)\n",
    "\n",
    "# --- Step 2: Create the mini dataset ---\n",
    "# Select the columns and drop duplicate rows\n",
    "df_models = df[['popmodCode', 'variantLabel', 'variant']].drop_duplicates().copy()\n",
    "\n",
    "# Rename columns: variantLabel -> popmodName, variant -> popmodDesc\n",
    "df_models = df_models.rename(columns={'variantLabel': 'popmodName', 'variant': 'popmodDesc'})\n",
    "\n",
    "# Save on local filesystem a copy of the df_final\n",
    "parquet_file_name = \"population_data_models.parquet\"\n",
    "parquet_file_path = os.path.join(folder_path, parquet_file_name)\n",
    "excel_file_name = \"population_data_models.xlsx\"\n",
    "excel_file_path = os.path.join(folder_path, excel_file_name)\n",
    "\n",
    "# Save only if not already there. Manually delete if you suspect you need to update it\n",
    "if not os.path.exists(parquet_file_path):    \n",
    "    print(f\"Saving (final) data to parquet at: {parquet_file_path}\")    \n",
    "    df_models.to_parquet(parquet_file_path, engine=\"pyarrow\")\n",
    "    print(f\"Saving (final) data to Excel at: {excel_file_path}\")    \n",
    "    df_models.to_excel(excel_file_path, index=False)\n",
    "\n",
    "df_models.reset_index(drop=True, inplace=True)\n",
    "# (Optional) Display the mini dataset\n",
    "df_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0897ae33-55bd-45c9-8b63-21fd4d34b231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mashup this data inline with how we use it in Pacific EMIS and save it.\n",
    "\n",
    "#df = pd.read_parquet(parquet_file_path, engine=\"pyarrow\")\n",
    "df.columns\n",
    "# of interest 'variantShortName', 'timeLabel', 'sex', 'ageLabel', 'ageStart', 'value'\n",
    "\n",
    "# Select the columns and rename them in one step\n",
    "selected_columns = ['variantShortName', 'timeLabel', 'sex', 'ageLabel', 'value']\n",
    "df_final = df[selected_columns].rename(columns={\n",
    "    #'variantShortName': 'popmodCode',\n",
    "    'timeLabel': 'popYear',\n",
    "    'sex': 'gender',\n",
    "    'ageLabel': 'popAge',\n",
    "    'value': 'pop'\n",
    "})\n",
    "\n",
    "# Ensure popYear is treated as an integer\n",
    "df_final['popYear'] = df_final['popYear'].astype(int)\n",
    "\n",
    "# Clean the 'popAge' column: remove '+' and convert to int\n",
    "df_final['popAge'] = df_final['popAge'].str.replace('+', '', regex=False).astype(int)\n",
    "\n",
    "# Clean the population column: round to an integer (remove decimals)\n",
    "df_final['pop'] = df_final['pop'].round().astype(int)\n",
    "\n",
    "# Create a pivot table so that genders become separate columns\n",
    "df_final = df_final.pivot_table(\n",
    "    index=[\"variantShortName\", \"popYear\", \"popAge\"],\n",
    "    columns=\"gender\",\n",
    "    values=\"pop\",\n",
    "    aggfunc=\"sum\"\n",
    ").reset_index()\n",
    "\n",
    "# Adjust the mapping based on how genders are labeled in data.\n",
    "gender_mapping = {\"Male\": \"popM\", \"Female\": \"popF\"}\n",
    "df_final.rename(columns=gender_mapping, inplace=True)\n",
    "\n",
    "# Drop Both sexes column, it contains small difference vs Male + Female which ultimately\n",
    "# will be what is used in the EMIS, so might as well line it up now.\n",
    "df_final.drop(\"Both sexes\", axis=1, inplace=True)\n",
    "\n",
    "# Rename columns for clarity\n",
    "df_final.columns = [\"variantShortName\", \"popYear\", \"popAge\", \"popF\", \"popM\"]\n",
    "\n",
    "# Ages should be int\n",
    "df_final[\"popM\"] = df_final[\"popM\"].round().astype(int)\n",
    "df_final[\"popF\"] = df_final[\"popF\"].round().astype(int)\n",
    "\n",
    "# Save on local filesystem a copy of the df_final\n",
    "parquet_file_name = \"population_data_final.parquet\"\n",
    "parquet_file_path = os.path.join(folder_path, parquet_file_name)\n",
    "excel_file_name = \"population_data_final.xlsx\"\n",
    "excel_file_path = os.path.join(folder_path, excel_file_name)\n",
    "\n",
    "# Make a minor modifications for saving into local filesystem (and eventually SQL DB)\n",
    "df_final_for_saving = df_final.copy()\n",
    "# Create the new popmodCode column using the dynamic mapping (use same mapping as previous cell)\n",
    "df_final_for_saving[\"popmodCode\"] = df_final_for_saving[\"variantShortName\"].map(mapping)\n",
    "df_final_for_saving.drop(\"variantShortName\", axis=1, inplace=True)\n",
    "#df_final_for_saving.columns = [\"popmodCode\", \"popYear\", \"popAge\", \"popF\", \"popM\"]\n",
    "df_final_for_saving = df_final_for_saving[[\"popmodCode\", \"popYear\", \"popAge\", \"popF\", \"popM\"]]\n",
    "\n",
    "# Save only if not already there. Manually delete if you suspect you need to update it\n",
    "if not os.path.exists(parquet_file_path):    \n",
    "    print(f\"Saving (final) data to parquet at: {parquet_file_path}\")\n",
    "    df_final_for_saving.to_parquet(parquet_file_path, engine=\"pyarrow\")\n",
    "    print(f\"Saving (final) data to Excel at: {excel_file_path}\")\n",
    "    df_final_for_saving.to_excel(excel_file_path, index=False)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9cf306-660a-48f6-bded-1e04db02aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a quick pivot table to compare with UN website table\n",
    "# https://population.un.org/dataportal/data/indicators/47/locations/584/start/2011/end/2030/table/pivotbylocation?df=4e173b6f-2262-4c28-b909-17788d1d8ca4\n",
    "\n",
    "# Sort by popYear (ascending) and popAge (ascending)\n",
    "df_sorted = df_final.sort_values(by=[\"popYear\", \"popAge\"], ascending=[True, True])\n",
    "\n",
    "# Reshape data for pivoting\n",
    "df_melted = df_sorted.melt(\n",
    "    id_vars=[\"popYear\", \"popAge\"], \n",
    "    value_vars=[\"popM\", \"popF\"], \n",
    "    var_name=\"Gender\", \n",
    "    value_name=\"Population\"\n",
    ")\n",
    "\n",
    "# Rename Gender values for clarity\n",
    "df_melted[\"Gender\"] = df_melted[\"Gender\"].replace({\"popM\": \"Male\", \"popF\": \"Female\"})\n",
    "\n",
    "# Create pivot table\n",
    "pivot_table = df_melted.pivot_table(\n",
    "    index=\"popYear\", \n",
    "    columns=[\"Gender\", \"popAge\"], \n",
    "    values=\"Population\",\n",
    "    aggfunc=\"sum\"\n",
    ")\n",
    "\n",
    "# Sort columns to ensure Age (0-100) is in order within each Gender\n",
    "pivot_table = pivot_table.sort_index(axis=1, level=[0, 1])\n",
    "\n",
    "# Display pivot table\n",
    "pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c929adb0-a6f3-44e1-b15f-c4e2c401db2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is likely a bit buggy but it it served its purposed at the time.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "# --- User Options ---\n",
    "# For variant_to_plot, possible options:\n",
    "#   \"All\"                       => Plot multiple series, one for each unique variant\n",
    "#   \"UNPD24V1\"                    => a single variant\n",
    "#   [\"UNPD24V1\", \"UNPD24V2\"] => multiple variants\n",
    "variant_to_plot = \"All\"  \n",
    "#variant_to_plot = \"UNPD24V1\"  \n",
    "#variant_to_plot = [\"UNPD24V1\", \"UNPD24V13\"]\n",
    "\n",
    "# For age_to_plot, possible options:\n",
    "#   \"All\"            => Aggregate all ages (single series)\n",
    "#   5                => A single age\n",
    "#   [5, 10, 15]      => Multiple ages (multiple series)\n",
    "#age_to_plot = [5, 10, 15]  \n",
    "age_to_plot = \"All\"\n",
    "#age_to_plot = 5\n",
    "\n",
    "# --- Preprocess Options ---\n",
    "# If variant_to_plot is \"All\", then convert it to a list of all unique variants.\n",
    "if variant_to_plot == \"All\":\n",
    "    variant_to_plot = df_final[\"variantShortName\"].unique().tolist()\n",
    "\n",
    "# Now, if both dimensions are multiple series, that's not allowed.\n",
    "if isinstance(age_to_plot, list) and isinstance(variant_to_plot, list):\n",
    "    raise ValueError(\"Cannot have both age_to_plot and variant_to_plot as multiple series. Choose one dimension to vary.\")\n",
    "\n",
    "# --- Filter by Variant ---\n",
    "if isinstance(variant_to_plot, list):\n",
    "    # Multiple variants: keep only rows that match these variants.\n",
    "    df_variant = df_final[df_final[\"variantShortName\"].isin(variant_to_plot)].copy()\n",
    "else:\n",
    "    df_variant = df_final[df_final[\"variantShortName\"] == variant_to_plot].copy()\n",
    "\n",
    "# --- Grouping and Pivoting ---\n",
    "# Case 1: Multiple series by Age\n",
    "if isinstance(age_to_plot, list):\n",
    "    # Filter for the selected ages.\n",
    "    df_filtered = df_variant[df_variant[\"popAge\"].isin(age_to_plot)].copy()\n",
    "    # Group by popYear and popAge so each age remains separate.\n",
    "    df_grouped = df_filtered.groupby([\"popYear\", \"popAge\"])[[\"popM\", \"popF\"]].sum().reset_index()\n",
    "    # Compute total population (in thousands).\n",
    "    df_grouped[\"popT\"] = (df_grouped[\"popM\"] + df_grouped[\"popF\"]) / 1000\n",
    "    # Pivot so that each age becomes its own column.\n",
    "    df_plot_data = df_grouped.pivot(index=\"popYear\", columns=\"popAge\", values=\"popT\")\n",
    "    series_label = \"Age\"\n",
    "\n",
    "# Case 2: Multiple series by Variant\n",
    "elif isinstance(variant_to_plot, list):\n",
    "    # Here, age_to_plot must be \"All\" or a single age.\n",
    "    if age_to_plot == \"All\":\n",
    "        df_grouped = df_variant.groupby([\"popYear\", \"variantShortName\"])[[\"popM\", \"popF\"]].sum().reset_index()\n",
    "    else:\n",
    "        df_filtered = df_variant[df_variant[\"popAge\"] == age_to_plot].copy()\n",
    "        df_grouped = df_filtered.groupby([\"popYear\", \"variantShortName\"])[[\"popM\", \"popF\"]].sum().reset_index()\n",
    "    df_grouped[\"popT\"] = (df_grouped[\"popM\"] + df_grouped[\"popF\"]) / 1000\n",
    "    df_plot_data = df_grouped.pivot(index=\"popYear\", columns=\"variantShortName\", values=\"popT\")\n",
    "    series_label = \"Variant\"\n",
    "\n",
    "# Case 3: Single series (both dimensions are single or age is \"All\")\n",
    "else:\n",
    "    if age_to_plot == \"All\":\n",
    "        df_plot_data = df_variant.groupby(\"popYear\")[[\"popM\", \"popF\"]].sum()\n",
    "    else:\n",
    "        df_plot_data = df_variant[df_variant[\"popAge\"] == age_to_plot].groupby(\"popYear\")[[\"popM\", \"popF\"]].sum()\n",
    "    df_plot_data[\"popT\"] = (df_plot_data[\"popM\"] + df_plot_data[\"popF\"]) / 1000\n",
    "\n",
    "# Ensure popYear is an integer type\n",
    "df_plot_data.index = df_plot_data.index.astype(int)\n",
    "\n",
    "# --- Split Data into Interpolated and Projected Segments ---\n",
    "# (Interpolated: years <= 2024, Projected: years >= 2024; 2024 appears in both for continuity)\n",
    "interpolated = df_plot_data.loc[df_plot_data.index <= 2024]\n",
    "projected = df_plot_data.loc[df_plot_data.index >= 2024]\n",
    "\n",
    "# --- Determine Y-axis Limit (25% above maximum) ---\n",
    "if isinstance(age_to_plot, list) or isinstance(variant_to_plot, list):\n",
    "    max_population = df_plot_data.max().max()\n",
    "else:\n",
    "    max_population = df_plot_data[\"popT\"].max()\n",
    "y_max = max_population * 1.25\n",
    "\n",
    "# --- Plotting ---\n",
    "# Double the chart size by setting a larger figure size.\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "# For multiple series, use the pivoted DataFrame columns as individual series.\n",
    "if isinstance(age_to_plot, list) or isinstance(variant_to_plot, list):\n",
    "    series_keys = df_plot_data.columns.tolist()\n",
    "    cmap = plt.get_cmap('tab10', len(series_keys))\n",
    "    for idx, key in enumerate(series_keys):\n",
    "        color = cmap(idx)\n",
    "        # Plot the Interpolated segment (solid line)\n",
    "        ax.plot(interpolated.index, interpolated[key],\n",
    "                label=f\"{series_label} {key} Interpolated\", color=color, linestyle=\"-\")\n",
    "        # Plot the Projected segment (dashed line, same color)\n",
    "        ax.plot(projected.index, projected[key],\n",
    "                label=f\"{series_label} {key} Projected\", color=color, linestyle=\"--\")\n",
    "else:\n",
    "    ax.plot(interpolated.index, interpolated[\"popT\"], label=\"Interpolated\", color=\"blue\", linestyle=\"-\")\n",
    "    ax.plot(projected.index, projected[\"popT\"], label=\"Projected\", color=\"blue\", linestyle=\"--\")\n",
    "\n",
    "ax.set_xticks(df_plot_data.index)\n",
    "ax.set_ylim(0, y_max)\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Population (thousands)\")\n",
    "\n",
    "# Build title information\n",
    "if isinstance(variant_to_plot, list):\n",
    "    variant_info = f\"Variants: {', '.join(variant_to_plot)}\"\n",
    "else:\n",
    "    variant_info = f\"Variant: {variant_to_plot}\"\n",
    "if isinstance(age_to_plot, list):\n",
    "    title_info = f\"Ages: {', '.join(map(str, age_to_plot))}\"\n",
    "else:\n",
    "    title_info = f\"Age: {age_to_plot}\" if age_to_plot != \"All\" else \"All Ages\"\n",
    "\n",
    "ax.set_title(f\"Population Trends ({variant_info}, {title_info})\")\n",
    "ax.grid(True)\n",
    "# Place the legend outside the plot on the right.\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79af24e-03b4-45ef-aefc-f8a0c29a610b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
